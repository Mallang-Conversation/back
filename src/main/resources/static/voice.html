<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <title>실시간 음성 대화</title>
</head>
<body>
<h1>실시간 음성 대화</h1>

<!-- 대화 관련 버튼 -->
<button id="conversationStartBtn">대화 시작</button>
<button id="conversationEndBtn" disabled>대화 종료</button>
<br><br>

<!-- 대화 입력 및 전송 -->
<input type="text" id="messageInput" placeholder="메시지를 입력하세요" disabled>
<button id="sendMessageBtn" disabled>전송</button>
<br><br>

<!-- 채팅 로그 -->
<div id="chatLog" style="border:1px solid #ccc; padding:10px; width: 300px; height: 200px; overflow-y: auto;"></div>
<br><br>

<!-- 녹음 관련 버튼 (웹소켓 연결되어야 활성화됨) -->
<button id="recordStartBtn" disabled>녹음 시작</button>
<button id="recordStopBtn" disabled>녹음 중지</button>

<script>
  let ws;
  let audioContext;
  let processor;
  let input;
  let mediaStream;
  let audioDataBuffer = []; // Int16Array 조각들을 누적

  // 버튼 요소
  const conversationStartBtn = document.getElementById("conversationStartBtn");
  const conversationEndBtn = document.getElementById("conversationEndBtn");
  const recordStartBtn = document.getElementById("recordStartBtn");
  const recordStopBtn = document.getElementById("recordStopBtn");
  const messageInput = document.getElementById("messageInput");
  const sendMessageBtn = document.getElementById("sendMessageBtn");
  const chatLog = document.getElementById("chatLog");

  // 대화 시작 버튼 클릭: 웹소켓(ws://localhost:8081/ws/chat) 연결
  conversationStartBtn.addEventListener("click", () => {
    ws = new WebSocket("ws://localhost:8081/ws/chat");
    ws.binaryType = "arraybuffer";

    ws.onopen = () => {
      console.log("대화용 WebSocket 연결 성공");
      recordStartBtn.disabled = false;
      messageInput.disabled = false;
      sendMessageBtn.disabled = false;
    };

    ws.onmessage = (event) => {
      const data = event.data;

      if (data.startsWith("SESSION_ID:")) {
        const sessionId = data.split(":")[1];
        console.log("세션 ID:", sessionId);
        window.sessionId = sessionId;
      } else {
        displayMessage("서버: " + data);
      }
    };

    ws.onerror = (err) => {
      console.error("WebSocket 에러:", err);
    };

    ws.onclose = () => {
      console.log("WebSocket 연결 종료");
      recordStartBtn.disabled = true;
      messageInput.disabled = true;
      sendMessageBtn.disabled = true;
    };

    conversationStartBtn.disabled = true;
    conversationEndBtn.disabled = false;
  });

  // 대화 종료 버튼 클릭: 웹소켓 연결 종료
  conversationEndBtn.addEventListener("click", () => {
    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.close();
    }
    conversationStartBtn.disabled = false;
    conversationEndBtn.disabled = true;
    recordStartBtn.disabled = true;
    messageInput.disabled = true;
    sendMessageBtn.disabled = true;
  });

  // 메시지 전송 버튼 클릭 시 서버로 메시지 전송
  sendMessageBtn.addEventListener("click", () => {
    const message = messageInput.value.trim();
    if (message && ws.readyState === WebSocket.OPEN) {
      ws.send(message);
      displayMessage("나: " + message);
      messageInput.value = ""; // 입력창 초기화
    }
  });

  // 메시지를 화면에 표시하는 함수
  function displayMessage(text) {
    const messageElement = document.createElement("div");
    messageElement.textContent = text;
    chatLog.appendChild(messageElement);
    chatLog.scrollTop = chatLog.scrollHeight; // 최신 메시지로 스크롤 이동
  }

  // 녹음 관련 로직 (기존 코드 유지)
  recordStartBtn.addEventListener("click", async () => {
    if (!ws || ws.readyState !== WebSocket.OPEN) {
      alert("대화가 연결되어 있어야 녹음을 시작할 수 있습니다.");
      return;
    }

    try {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      input = audioContext.createMediaStreamSource(mediaStream);

      processor = audioContext.createScriptProcessor(4096, 1, 1);
      input.connect(processor);
      processor.connect(audioContext.destination);

      processor.onaudioprocess = (e) => {
        const channelData = e.inputBuffer.getChannelData(0);
        const pcmChunk = floatTo16BitPCM(channelData);
        audioDataBuffer.push(pcmChunk);

        if (ws.readyState === WebSocket.OPEN) {
          ws.send(pcmChunk.buffer);
        }
      };

      console.log("PCM 데이터 수집 시작");
      recordStartBtn.disabled = true;
      recordStopBtn.disabled = false;
    } catch (err) {
      console.error("마이크 접근 또는 AudioContext 생성 에러:", err);
    }
  });

  recordStopBtn.addEventListener("click", () => {
    if (processor) {
      processor.disconnect();
      if (input) input.disconnect();
      processor.onaudioprocess = null;
    }
    if (audioContext) audioContext.close();
    if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());

    const mergedPCM = mergeBuffers(audioDataBuffer);
    const wavBlob = createWavBlob(mergedPCM, 44100, 1, 16);

    const url = URL.createObjectURL(wavBlob);
    const a = document.createElement("a");
    a.href = url;
    a.download = `${window.sessionId}.wav`;
    document.body.appendChild(a);
    a.click();
    window.URL.revokeObjectURL(url);

    audioDataBuffer = [];
    recordStartBtn.disabled = false;
    recordStopBtn.disabled = true;
  });

  function floatTo16BitPCM(float32Array) {
    const len = float32Array.length;
    const int16Array = new Int16Array(len);
    for (let i = 0; i < len; i++) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
  }

  function mergeBuffers(buffers) {
    let totalLength = buffers.reduce((acc, curr) => acc + curr.length, 0);
    const result = new Int16Array(totalLength);
    let offset = 0;
    buffers.forEach(buffer => {
      result.set(buffer, offset);
      offset += buffer.length;
    });
    return result;
  }

  function createWavBlob(pcmData, sampleRate, channels, bitsPerSample) {
    const header = new ArrayBuffer(44);
    const view = new DataView(header);

    writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + pcmData.byteLength * (bitsPerSample / 8), true);
    writeString(view, 8, 'WAVE');
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true);             // Subchunk1Size (16 for PCM)
    view.setUint16(20, 1, true);              // AudioFormat (1 = PCM)
    view.setUint16(22, channels, true);       // NumChannels
    view.setUint32(24, sampleRate, true);     // SampleRate
    view.setUint32(28, sampleRate * channels * bitsPerSample / 8, true); // ByteRate
    view.setUint16(32, channels * bitsPerSample / 8, true); // BlockAlign
    view.setUint16(34, bitsPerSample, true);  // BitsPerSample
    writeString(view, 36, 'data');
    view.setUint32(40, pcmData.byteLength * (bitsPerSample / 8), true);

    const pcmBuffer = pcmData.buffer;
    const wavBuffer = new Uint8Array(header.byteLength + pcmBuffer.byteLength);
    wavBuffer.set(new Uint8Array(header), 0);
    wavBuffer.set(new Uint8Array(pcmBuffer), header.byteLength);

    return new Blob([wavBuffer], { type: 'audio/wav' });
  }

  function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }
</script>
</body>
</html>